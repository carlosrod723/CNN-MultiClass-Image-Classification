{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWt6Rxzhy3DB",
        "outputId": "ef384164-4d76-495f-aa49-7dc065c66deb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries and packages\n",
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPool2D, Flatten, Dropout, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.utils.class_weight import compute_class_weight"
      ],
      "metadata": {
        "id": "54Anwxwlz-UC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set directory path\n",
        "data_dir= '/content/drive/My Drive/CNN-MultiClass-Classification/Data/training_data'"
      ],
      "metadata": {
        "id": "0Hu8x6I21IQn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to get the data from the directory\n",
        "def get_data(data_dir, img_size=224):\n",
        "    '''\n",
        "    Loads images from the directory and assigns labels according to their folder name.\n",
        "    Args:\n",
        "    - data_dir: Directory where all classes will have folders with images.\n",
        "    - img_size: Size to which images will be resized\n",
        "\n",
        "    Returns:\n",
        "    - images: A list of the image arrays.\n",
        "    - labels: A list of corresponding labels.\n",
        "    '''\n",
        "\n",
        "    # List of class labels\n",
        "    labels= ['driving_license', 'social_security', 'others']\n",
        "\n",
        "    # Lists to store the image data and labels separately\n",
        "    images= []\n",
        "    image_labels= []\n",
        "\n",
        "    # Loop through each class label folder and load the images\n",
        "    for label in labels:\n",
        "        path= os.path.join(data_dir, label)\n",
        "        class_num= labels.index(label)\n",
        "\n",
        "        for img in os.listdir(path):\n",
        "            try:\n",
        "\n",
        "                # Convert BGR to RGB\n",
        "                img_arr= cv2.imread(os.path.join(path, img))[..., ::-1]\n",
        "\n",
        "                # Resize the image\n",
        "                resized_arr= cv2.resize(img_arr, (img_size, img_size))\n",
        "\n",
        "                # Append the image to the list\n",
        "                images.append(resized_arr)\n",
        "\n",
        "                # Append the label (0, 1, or 2)\n",
        "                image_labels.append(class_num)\n",
        "            except Exception as e:\n",
        "                print(f'Error loading image {img}: {e}')\n",
        "\n",
        "    return np.array(images), np.array(image_labels)"
      ],
      "metadata": {
        "id": "RdIMlTCs1RQV"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the ImageDataGenerator for data augmentation and split the data\n",
        "datagen= ImageDataGenerator(\n",
        "    rotation_range= 45,\n",
        "    zoom_range= 0.3,\n",
        "    width_shift_range= 0.2,\n",
        "    height_shift_range= 0.2,\n",
        "    horizontal_flip= True,\n",
        "    validation_split= 0.2\n",
        ")"
      ],
      "metadata": {
        "id": "Aq263PcD32RF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset. X contains the images, y contains the labels\n",
        "X, y= get_data(data_dir)\n",
        "\n",
        "# Normalize the image data to range [0,1]\n",
        "X = X / 255.0\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val= train_test_split(X, y, test_size= 0.2, stratify= y, random_state= 42)\n",
        "\n",
        "# Verify the shapes of the datasets\n",
        "print(f'X_train Shape: {X_train.shape}, y_train Shape: {y_train.shape}')\n",
        "print(f'X_val Shape: {X_val.shape}, y_val Shape: {y_val.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcBSCZmo7zj0",
        "outputId": "2a33679c-98ab-4a6f-fe73-8f6bf9c10a6d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train Shape: (480, 224, 224, 3), y_train Shape: (480,)\n",
            "X_val Shape: (120, 224, 224, 3), y_val Shape: (120,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Summary:\n",
        "\n",
        "- **X_train Shape**: (480, 224, 224, 3) — 480 training images (224x224, RGB), where `3` is the number of color channels.\n",
        "- **y_train Shape**: (480,) — 480 corresponding labels for training images (3 classes: driving_license, social_security, others).\n",
        "- **X_val Shape**: (120, 224, 224, 3) — 120 validation images (224x224, RGB), where `3` is the number of color channels.\n",
        "- **y_val Shape**: (120,) — 120 corresponding labels for validation images (3 classes: driving_license, social_security, others).\n"
      ],
      "metadata": {
        "id": "tXfsYDEWFk1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the CNN model\n",
        "model= Sequential()\n",
        "\n",
        "# Input layer\n",
        "model.add(Input(shape= (224,224,3)))\n",
        "\n",
        "# First convolutional layer\n",
        "model.add(Conv2D(32, 3, padding= 'same', activation= 'relu'))\n",
        "\n",
        "# Batch normalization to normalize the inputs to the next layer\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Max pooling layer to reduce spacial dimensions of the feature map\n",
        "model.add(MaxPool2D())\n",
        "\n",
        "# Dropout layer to prevent overfitting\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "# Flatten the feature map into a 1D vector for the fully connected layer\n",
        "model.add(Flatten())\n",
        "\n",
        "# Fully connected layer\n",
        "model.add(Dense(128, activation= 'relu'))\n",
        "\n",
        "# Output layer with 3 units (one for each class) and softmax activation for multi-class classification\n",
        "model.add(Dense(3, activation= 'softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer= Adam(learning_rate= 0.00001), loss= tf.keras.losses.SparseCategoricalCrossentropy(from_logits= True), metrics= ['accuracy'])"
      ],
      "metadata": {
        "id": "UyHzI-yd4UZx"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the class weights\n",
        "class_weights= compute_class_weight('balanced', classes= np.unique(y_train), y= y_train)\n",
        "\n",
        "# Convert to a dictionary format as required by Keras\n",
        "class_weights_dict= {i: class_weights[i] for i in range(len(class_weights))}\n",
        "\n",
        "print(f'Class Weights; {class_weights_dict}')\n",
        "\n",
        "# Use the ImageDataGenerator to fit the model\n",
        "train_gen= datagen.flow(X_train, y_train, batch_size= 32, subset= 'training')\n",
        "val_gen= datagen.flow(X_val, y_val, batch_size= 32, subset= 'validation')\n",
        "\n",
        "# Early stopping\n",
        "early_stop= EarlyStopping(monitor= 'val_loss', patience= 10, restore_best_weights= True)\n",
        "\n",
        "# Learning rate scheduler\n",
        "lr_scheduler= ReduceLROnPlateau(monitor= 'val_loss', factor= 0.1, patience= 5)\n",
        "\n",
        "# Train the model\n",
        "history= model.fit(train_gen, epochs= 200, validation_data= val_gen, callbacks= [early_stop, lr_scheduler], class_weight= class_weights_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttvpELq46m1_",
        "outputId": "25d11662-a84e-41bc-a8a4-c622e41a15c3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Weights; {0: 1.0, 1: 1.0, 2: 1.0}\n",
            "Epoch 1/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/nn.py:609: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 251ms/step - accuracy: 0.4542 - loss: 1.4056 - val_accuracy: 0.2917 - val_loss: 1.9146 - learning_rate: 1.0000e-05\n",
            "Epoch 2/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 172ms/step - accuracy: 0.5574 - loss: 1.0808 - val_accuracy: 0.2917 - val_loss: 1.5614 - learning_rate: 1.0000e-05\n",
            "Epoch 3/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 153ms/step - accuracy: 0.6495 - loss: 0.8894 - val_accuracy: 0.2917 - val_loss: 1.6398 - learning_rate: 1.0000e-05\n",
            "Epoch 4/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 179ms/step - accuracy: 0.6821 - loss: 0.8143 - val_accuracy: 0.2917 - val_loss: 1.5118 - learning_rate: 1.0000e-05\n",
            "Epoch 5/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 149ms/step - accuracy: 0.7053 - loss: 0.6833 - val_accuracy: 0.2917 - val_loss: 1.7403 - learning_rate: 1.0000e-05\n",
            "Epoch 6/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 150ms/step - accuracy: 0.7027 - loss: 0.7796 - val_accuracy: 0.2917 - val_loss: 1.5645 - learning_rate: 1.0000e-05\n",
            "Epoch 7/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 153ms/step - accuracy: 0.6939 - loss: 0.7276 - val_accuracy: 0.2917 - val_loss: 1.8646 - learning_rate: 1.0000e-05\n",
            "Epoch 8/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 154ms/step - accuracy: 0.7598 - loss: 0.6366 - val_accuracy: 0.2917 - val_loss: 1.5804 - learning_rate: 1.0000e-05\n",
            "Epoch 9/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 153ms/step - accuracy: 0.7255 - loss: 0.6363 - val_accuracy: 0.2917 - val_loss: 1.5131 - learning_rate: 1.0000e-05\n",
            "Epoch 10/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 177ms/step - accuracy: 0.7650 - loss: 0.6252 - val_accuracy: 0.3333 - val_loss: 1.4450 - learning_rate: 1.0000e-06\n",
            "Epoch 11/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 159ms/step - accuracy: 0.7479 - loss: 0.6232 - val_accuracy: 0.3333 - val_loss: 1.5696 - learning_rate: 1.0000e-06\n",
            "Epoch 12/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 174ms/step - accuracy: 0.6989 - loss: 0.6654 - val_accuracy: 0.3333 - val_loss: 1.4185 - learning_rate: 1.0000e-06\n",
            "Epoch 13/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 174ms/step - accuracy: 0.7841 - loss: 0.5129 - val_accuracy: 0.3333 - val_loss: 1.2614 - learning_rate: 1.0000e-06\n",
            "Epoch 14/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 171ms/step - accuracy: 0.7973 - loss: 0.5019 - val_accuracy: 0.4167 - val_loss: 1.2094 - learning_rate: 1.0000e-06\n",
            "Epoch 15/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 172ms/step - accuracy: 0.8202 - loss: 0.4962 - val_accuracy: 0.3333 - val_loss: 1.1571 - learning_rate: 1.0000e-06\n",
            "Epoch 16/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 152ms/step - accuracy: 0.8367 - loss: 0.4527 - val_accuracy: 0.3750 - val_loss: 1.1891 - learning_rate: 1.0000e-06\n",
            "Epoch 17/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 151ms/step - accuracy: 0.7701 - loss: 0.5577 - val_accuracy: 0.3750 - val_loss: 1.1619 - learning_rate: 1.0000e-06\n",
            "Epoch 18/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 171ms/step - accuracy: 0.7348 - loss: 0.5835 - val_accuracy: 0.3333 - val_loss: 1.0918 - learning_rate: 1.0000e-06\n",
            "Epoch 19/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 153ms/step - accuracy: 0.7676 - loss: 0.5618 - val_accuracy: 0.4167 - val_loss: 1.1270 - learning_rate: 1.0000e-06\n",
            "Epoch 20/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 151ms/step - accuracy: 0.7334 - loss: 0.6030 - val_accuracy: 0.2917 - val_loss: 1.1167 - learning_rate: 1.0000e-06\n",
            "Epoch 21/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 173ms/step - accuracy: 0.7348 - loss: 0.6009 - val_accuracy: 0.4167 - val_loss: 0.9825 - learning_rate: 1.0000e-06\n",
            "Epoch 22/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 151ms/step - accuracy: 0.7994 - loss: 0.5104 - val_accuracy: 0.3750 - val_loss: 1.1054 - learning_rate: 1.0000e-06\n",
            "Epoch 23/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 172ms/step - accuracy: 0.7484 - loss: 0.6241 - val_accuracy: 0.5000 - val_loss: 0.9309 - learning_rate: 1.0000e-06\n",
            "Epoch 24/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 152ms/step - accuracy: 0.7737 - loss: 0.5127 - val_accuracy: 0.4583 - val_loss: 1.0512 - learning_rate: 1.0000e-06\n",
            "Epoch 25/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 152ms/step - accuracy: 0.7892 - loss: 0.5332 - val_accuracy: 0.4583 - val_loss: 1.0475 - learning_rate: 1.0000e-06\n",
            "Epoch 26/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 152ms/step - accuracy: 0.7663 - loss: 0.5087 - val_accuracy: 0.5833 - val_loss: 0.9475 - learning_rate: 1.0000e-06\n",
            "Epoch 27/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 154ms/step - accuracy: 0.7828 - loss: 0.5135 - val_accuracy: 0.5833 - val_loss: 0.9911 - learning_rate: 1.0000e-06\n",
            "Epoch 28/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 155ms/step - accuracy: 0.7524 - loss: 0.5891 - val_accuracy: 0.5417 - val_loss: 0.9350 - learning_rate: 1.0000e-06\n",
            "Epoch 29/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 174ms/step - accuracy: 0.8092 - loss: 0.5025 - val_accuracy: 0.5000 - val_loss: 0.8483 - learning_rate: 1.0000e-07\n",
            "Epoch 30/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 153ms/step - accuracy: 0.7887 - loss: 0.5197 - val_accuracy: 0.4583 - val_loss: 0.9769 - learning_rate: 1.0000e-07\n",
            "Epoch 31/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 153ms/step - accuracy: 0.7721 - loss: 0.5501 - val_accuracy: 0.5417 - val_loss: 0.9275 - learning_rate: 1.0000e-07\n",
            "Epoch 32/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 172ms/step - accuracy: 0.7876 - loss: 0.4838 - val_accuracy: 0.7083 - val_loss: 0.7309 - learning_rate: 1.0000e-07\n",
            "Epoch 33/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 153ms/step - accuracy: 0.7353 - loss: 0.5667 - val_accuracy: 0.6250 - val_loss: 0.7663 - learning_rate: 1.0000e-07\n",
            "Epoch 34/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 155ms/step - accuracy: 0.8317 - loss: 0.4337 - val_accuracy: 0.6250 - val_loss: 0.7594 - learning_rate: 1.0000e-07\n",
            "Epoch 35/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 152ms/step - accuracy: 0.7546 - loss: 0.5341 - val_accuracy: 0.6667 - val_loss: 0.7662 - learning_rate: 1.0000e-07\n",
            "Epoch 36/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 153ms/step - accuracy: 0.7377 - loss: 0.5929 - val_accuracy: 0.6250 - val_loss: 0.7819 - learning_rate: 1.0000e-07\n",
            "Epoch 37/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 152ms/step - accuracy: 0.7630 - loss: 0.5405 - val_accuracy: 0.6250 - val_loss: 0.7453 - learning_rate: 1.0000e-07\n",
            "Epoch 38/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 156ms/step - accuracy: 0.8415 - loss: 0.4267 - val_accuracy: 0.6250 - val_loss: 0.8262 - learning_rate: 1.0000e-08\n",
            "Epoch 39/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 177ms/step - accuracy: 0.7879 - loss: 0.5603 - val_accuracy: 0.7083 - val_loss: 0.6936 - learning_rate: 1.0000e-08\n",
            "Epoch 40/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 154ms/step - accuracy: 0.7600 - loss: 0.5726 - val_accuracy: 0.7083 - val_loss: 0.7612 - learning_rate: 1.0000e-08\n",
            "Epoch 41/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 152ms/step - accuracy: 0.8168 - loss: 0.4852 - val_accuracy: 0.6250 - val_loss: 0.8889 - learning_rate: 1.0000e-08\n",
            "Epoch 42/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 153ms/step - accuracy: 0.7865 - loss: 0.4948 - val_accuracy: 0.6667 - val_loss: 0.7406 - learning_rate: 1.0000e-08\n",
            "Epoch 43/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 172ms/step - accuracy: 0.7675 - loss: 0.5108 - val_accuracy: 0.7083 - val_loss: 0.6447 - learning_rate: 1.0000e-08\n",
            "Epoch 44/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 154ms/step - accuracy: 0.8045 - loss: 0.4915 - val_accuracy: 0.7083 - val_loss: 0.7486 - learning_rate: 1.0000e-08\n",
            "Epoch 45/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 157ms/step - accuracy: 0.8161 - loss: 0.4970 - val_accuracy: 0.6250 - val_loss: 0.8883 - learning_rate: 1.0000e-08\n",
            "Epoch 46/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 150ms/step - accuracy: 0.7631 - loss: 0.5084 - val_accuracy: 0.7500 - val_loss: 0.6632 - learning_rate: 1.0000e-08\n",
            "Epoch 47/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 171ms/step - accuracy: 0.8159 - loss: 0.4773 - val_accuracy: 0.7917 - val_loss: 0.5914 - learning_rate: 1.0000e-08\n",
            "Epoch 48/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 157ms/step - accuracy: 0.8034 - loss: 0.4571 - val_accuracy: 0.5417 - val_loss: 0.8692 - learning_rate: 1.0000e-08\n",
            "Epoch 49/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 177ms/step - accuracy: 0.7851 - loss: 0.5298 - val_accuracy: 0.7917 - val_loss: 0.5827 - learning_rate: 1.0000e-08\n",
            "Epoch 50/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 152ms/step - accuracy: 0.7933 - loss: 0.4462 - val_accuracy: 0.6667 - val_loss: 0.6521 - learning_rate: 1.0000e-08\n",
            "Epoch 51/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 155ms/step - accuracy: 0.7895 - loss: 0.5073 - val_accuracy: 0.6667 - val_loss: 0.9314 - learning_rate: 1.0000e-08\n",
            "Epoch 52/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 152ms/step - accuracy: 0.8208 - loss: 0.4717 - val_accuracy: 0.6667 - val_loss: 0.6856 - learning_rate: 1.0000e-08\n",
            "Epoch 53/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 178ms/step - accuracy: 0.7943 - loss: 0.4761 - val_accuracy: 0.6667 - val_loss: 0.5800 - learning_rate: 1.0000e-08\n",
            "Epoch 54/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 158ms/step - accuracy: 0.7559 - loss: 0.5625 - val_accuracy: 0.7500 - val_loss: 0.6591 - learning_rate: 1.0000e-08\n",
            "Epoch 55/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 153ms/step - accuracy: 0.7933 - loss: 0.4911 - val_accuracy: 0.7917 - val_loss: 0.6634 - learning_rate: 1.0000e-08\n",
            "Epoch 56/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 156ms/step - accuracy: 0.7958 - loss: 0.4990 - val_accuracy: 0.7500 - val_loss: 0.6055 - learning_rate: 1.0000e-08\n",
            "Epoch 57/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 154ms/step - accuracy: 0.7907 - loss: 0.4741 - val_accuracy: 0.6250 - val_loss: 0.7378 - learning_rate: 1.0000e-08\n",
            "Epoch 58/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 152ms/step - accuracy: 0.8067 - loss: 0.5211 - val_accuracy: 0.7083 - val_loss: 0.6883 - learning_rate: 1.0000e-08\n",
            "Epoch 59/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 151ms/step - accuracy: 0.7942 - loss: 0.5009 - val_accuracy: 0.7083 - val_loss: 0.6867 - learning_rate: 1.0000e-09\n",
            "Epoch 60/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 151ms/step - accuracy: 0.7953 - loss: 0.5238 - val_accuracy: 0.7500 - val_loss: 0.7602 - learning_rate: 1.0000e-09\n",
            "Epoch 61/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 151ms/step - accuracy: 0.7785 - loss: 0.5314 - val_accuracy: 0.7917 - val_loss: 0.6346 - learning_rate: 1.0000e-09\n",
            "Epoch 62/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 152ms/step - accuracy: 0.8293 - loss: 0.4658 - val_accuracy: 0.7500 - val_loss: 0.6290 - learning_rate: 1.0000e-09\n",
            "Epoch 63/200\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 152ms/step - accuracy: 0.7869 - loss: 0.5496 - val_accuracy: 0.6250 - val_loss: 0.8314 - learning_rate: 1.0000e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the validation data\n",
        "val_preds= np.argmax(model.predict(X_val), axis= -1)\n",
        "\n",
        "# Print classification report and confusion matrix\n",
        "print(classification_report(y_val, val_preds, target_names= ['driving_license', 'social_security', 'others']))\n",
        "print(confusion_matrix(y_val, val_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ya7pAy5J-llT",
        "outputId": "7b3d59d2-175f-420a-cb82-f14f7efbe719"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "driving_license       0.89      0.85      0.87        40\n",
            "social_security       0.76      0.95      0.84        40\n",
            "         others       0.97      0.78      0.86        40\n",
            "\n",
            "       accuracy                           0.86       120\n",
            "      macro avg       0.87      0.86      0.86       120\n",
            "   weighted avg       0.87      0.86      0.86       120\n",
            "\n",
            "[[34  5  1]\n",
            " [ 2 38  0]\n",
            " [ 2  7 31]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Model Performance Analysis:\n",
        "\n",
        "The model's performance on the validation set is summarized below:\n",
        "\n",
        "- **Overall Accuracy**: The model achieved an accuracy of **86%** on the validation set. This indicates that 86% of the predictions made by the model were correct.\n",
        "\n",
        "#### Class-wise Performance:\n",
        "1. **Driving License**:\n",
        "   - **Precision**: 0.89 — The model correctly identified 89% of the `driving_license` cases it predicted. A few misclassifications occurred, but the precision is strong.\n",
        "   - **Recall**: 0.85 — The model was able to correctly detect 85% of all actual `driving_license` cases. This indicates a slight miss in identifying some of the true positive cases.\n",
        "   - **F1-Score**: 0.87 — The balance between precision and recall for this class is very good, reflecting a solid overall performance.\n",
        "\n",
        "2. **Social Security**:\n",
        "   - **Precision**: 0.76 — The model is less confident in predicting `social_security`, as 24% of the cases it predicted were incorrect (false positives).\n",
        "   - **Recall**: 0.95 — The recall is very high, meaning the model successfully identified 95% of the actual `social_security` cases. This shows a strong ability to detect most true positives, though the lower precision suggests it is still misclassifying some other classes as `social_security`.\n",
        "   - **F1-Score**: 0.84 — Despite the lower precision, the F1-score remains strong due to the high recall.\n",
        "\n",
        "3. **Others**:\n",
        "   - **Precision**: 0.97 — The model was highly accurate in predicting `others` when it made a prediction, with only a few false positives.\n",
        "   - **Recall**: 0.78 — The model correctly identified 78% of all actual `others` cases. However, it missed some true cases, as indicated by the lower recall.\n",
        "   - **F1-Score**: 0.86 — A balanced performance overall for the `others` class, but there is still room for improvement in recall.\n",
        "\n",
        "#### Confusion Matrix:\n",
        "The confusion matrix provides further insight into the model’s performance:\n",
        "\n",
        "\\[\n",
        "\\begin{bmatrix}\n",
        "  34 & 5 & 1 \\\\\n",
        "  2 & 38 & 0 \\\\\n",
        "  2 & 7 & 31 \\\\\n",
        "\\end{bmatrix}\n",
        "\\]\n",
        "\n",
        "- The diagonal values (34, 38, and 31) represent the number of correct predictions for each class.\n",
        "- The model misclassified 5 instances of `driving_license` as `social_security` and 1 as `others`.\n",
        "- Similarly, 7 instances of `others` were misclassified as `social_security`.\n",
        "\n",
        "### Conclusion:\n",
        "The final model performs well overall, with strong accuracy across all classes. The model has a high recall for the `social_security` class but slightly lower precision, indicating room for improvement in minimizing false positives for this class. The `others` class also shows strong precision, but its recall could be further improved.\n",
        "\n"
      ],
      "metadata": {
        "id": "rwU7XMIOPJSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "model.save('model.keras')"
      ],
      "metadata": {
        "id": "iI6DhirA9xcE"
      },
      "execution_count": 29,
      "outputs": []
    }
  ]
}